import json
import argparse
import os
import glob
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from datetime import datetime
from emotion_analysis import LLMEmotionAnalyzer

class BatchEmotionProcessor:
    def __init__(self, config, max_workers=4):
        self.config = config
        self.max_workers = max_workers
        self.lock = threading.Lock()
        self.processed_count = 0
        self.success_count = 0
        self.failed_count = 0
        self.reprocessed_count = 0  # æ–°å¢ï¼šé‡æ–°å¤„ç†çš„æ–‡ä»¶è®¡æ•°
        self.start_time = None
        
    def load_config(self, config_file='emovoice_api_config.json'):
        """åŠ è½½APIé…ç½®"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œè¯·åˆ›å»ºé…ç½®æ–‡ä»¶")
            return None
        except json.JSONDecodeError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} æ ¼å¼é”™è¯¯")
            return None
    
    def check_emotion_analysis_success(self, detailed_file_path):
        """æ£€æŸ¥æƒ…ç»ªåˆ†æç»“æœæ–‡ä»¶æ˜¯å¦å¤„ç†æˆåŠŸ
        
        Args:
            detailed_file_path: è¯¦ç»†ç»“æœJSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: Trueè¡¨ç¤ºå¤„ç†æˆåŠŸï¼ŒFalseè¡¨ç¤ºéœ€è¦é‡æ–°å¤„ç†
        """
        try:
            if not detailed_file_path.exists():
                return False
                
            with open(detailed_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # æ£€æŸ¥processed_linesæ˜¯å¦ä¸º0
            processed_lines = data.get('processed_lines', 0)
            total_lines = data.get('total_lines', 0)
            
            # å¦‚æœprocessed_linesä¸º0ï¼Œè¯´æ˜å¤„ç†å¤±è´¥
            if processed_lines == 0:
                print(f"ğŸ”„ å‘ç°å¤±è´¥çš„åˆ†ææ–‡ä»¶ï¼ˆprocessed_lines=0ï¼‰: {detailed_file_path.name}")
                return False
                
            # å¦‚æœresultsæ•°ç»„ä¸ºç©ºï¼Œä¹Ÿè®¤ä¸ºæ˜¯å¤±è´¥
            results = data.get('results', [])
            if not results or len(results) == 0:
                print(f"ğŸ”„ å‘ç°ç©ºç»“æœçš„åˆ†ææ–‡ä»¶: {detailed_file_path.name}")
                return False
                
            return True
            
        except Exception as e:
            print(f"âš ï¸  æ£€æŸ¥æ–‡ä»¶æ—¶å‡ºé”™ {detailed_file_path}: {e}")
            return False
    
    def find_all_json_files(self, root_folder, pattern="*.json"):
        """æŸ¥æ‰¾æŒ‡å®šæ–‡ä»¶å¤¹ç»“æ„ä¸­çš„jsonæ–‡ä»¶
        
        æ–°è§„åˆ™ï¼š
        - æ‰«æ root_folder/å­æ–‡ä»¶å¤¹/azero_full*_analysis/ ä¸­çš„jsonæ–‡ä»¶
        - è·³è¿‡å·²ç»æ˜¯æƒ…ç»ªåˆ†æç»“æœçš„æ–‡ä»¶
        """
        json_files = []
        root_path = Path(root_folder)
        
        # éå†æ ¹æ–‡ä»¶å¤¹ä¸‹çš„ç›´æ¥å­æ–‡ä»¶å¤¹ï¼ˆå¦‚ test1/1, test1/2ï¼‰
        for level1_item in root_path.iterdir():
            if level1_item.is_dir():
                # ç›´æ¥åœ¨ä¸€çº§å­æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾ä»¥ azero_full å¼€å¤´ã€_analysis ç»“å°¾çš„æ–‡ä»¶å¤¹
                for level2_item in level1_item.iterdir():
                    if (level2_item.is_dir() and 
                        level2_item.name.startswith('azero_full') and 
                        level2_item.name.endswith('_analysis')):
                        # åœ¨ analysis æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾ json æ–‡ä»¶
                        for json_file in level2_item.glob(pattern):
                            if json_file.is_file():
                                # è·³è¿‡å·²ç»æ˜¯æƒ…ç»ªåˆ†æç»“æœçš„æ–‡ä»¶
                                if "_emotion_analysis" not in json_file.name:
                                    json_files.append(json_file)
                                    print(f"æ‰¾åˆ°æ–‡ä»¶: {json_file.relative_to(root_path)}")
        
        return json_files
    
    def get_output_filename(self, input_path, suffix="_emotion_analysis"):
        """æ ¹æ®è¾“å…¥æ–‡ä»¶ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å - è¾“å‡ºåˆ°åŒä¸€æ–‡ä»¶å¤¹"""
        input_path = Path(input_path)
        base_name = input_path.stem
        output_dir = input_path.parent  # ç›´æ¥ä½¿ç”¨jsonæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
        
        return {
            'summary_file': output_dir / f"{base_name}{suffix}_summary.txt",
            'detailed_file': output_dir / f"{base_name}{suffix}_detailed.json"
        }
    
    def is_large_file(self, file_path, max_lines=1000):
        """æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦åŒ…å«è¿‡å¤šè¡Œæ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return len(data) > max_lines
                elif isinstance(data, dict) and 'lines' in data:
                    return len(data['lines']) > max_lines
                else:
                    return False
        except Exception:
            return False
    
    def process_single_file(self, file_path, thread_id=1):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            with self.lock:
                self.processed_count += 1
                current_count = self.processed_count
            
            print(f"[çº¿ç¨‹{thread_id}] [{current_count}] å¼€å§‹å¤„ç†: {file_path.name}")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å¤„ç†æˆåŠŸ
            output_files = self.get_output_filename(file_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¤„ç†
            need_reprocess = False
            if output_files['summary_file'].exists():
                # æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦å¤„ç†æˆåŠŸ
                if self.check_emotion_analysis_success(output_files['detailed_file']):
                    print(f"[çº¿ç¨‹{thread_id}] [{current_count}] â­ï¸  è·³è¿‡ï¼ˆå·²æˆåŠŸå¤„ç†ï¼‰: {file_path.name}")
                    with self.lock:
                        self.success_count += 1
                    return True, f"å·²å­˜åœ¨ä¸”æˆåŠŸ: {file_path}"
                else:
                    need_reprocess = True
                    with self.lock:
                        self.reprocessed_count += 1
                    print(f"[çº¿ç¨‹{thread_id}] [{current_count}] ğŸ”„ é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ä»¶: {file_path.name}")
            
            # åˆ›å»ºæƒ…ç»ªåˆ†æå™¨
            analyzer = LLMEmotionAnalyzer(
                file_path=str(file_path),
                emovoice_api_config=self.config
            )
            
            # è¿è¡Œæƒ…ç»ªåˆ†æ
            result = analyzer.run_emotion_analysis()
            
            if result:
                with self.lock:
                    self.success_count += 1
                status_msg = "é‡æ–°å¤„ç†å®Œæˆ" if need_reprocess else "å®Œæˆ"
                print(f"[çº¿ç¨‹{thread_id}] [{current_count}] âœ… {status_msg}: {file_path.name}")
                return True, f"æˆåŠŸ: {file_path}"
            else:
                with self.lock:
                    self.failed_count += 1
                status_msg = "é‡æ–°å¤„ç†å¤±è´¥" if need_reprocess else "å¤±è´¥"
                print(f"[çº¿ç¨‹{thread_id}] [{current_count}] âŒ {status_msg}: {file_path.name}")
                return False, f"å¤±è´¥: {file_path}"
                
        except Exception as e:
            with self.lock:
                self.failed_count += 1
            error_msg = f"å¼‚å¸¸: {file_path} - {str(e)}"
            print(f"[çº¿ç¨‹{thread_id}] [{current_count}] ğŸ’¥ {error_msg}")
            return False, error_msg
    
    def print_progress(self, total_files):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        while self.processed_count < total_files:
            with self.lock:
                processed = self.processed_count
                success = self.success_count
                failed = self.failed_count
                reprocessed = self.reprocessed_count
            
            if self.start_time:
                elapsed = time.time() - self.start_time
                if processed > 0:
                    avg_time = elapsed / processed
                    remaining = (total_files - processed) * avg_time
                    eta = datetime.now().timestamp() + remaining
                    eta_str = datetime.fromtimestamp(eta).strftime('%H:%M:%S')
                else:
                    eta_str = "è®¡ç®—ä¸­..."
            else:
                eta_str = "è®¡ç®—ä¸­..."
            
            progress = (processed / total_files) * 100 if total_files > 0 else 0
            print(f"\rğŸ“Š è¿›åº¦: {processed}/{total_files} ({progress:.1f}%) | âœ…{success} âŒ{failed} ğŸ”„{reprocessed} | ETA: {eta_str}", end="", flush=True)
            time.sleep(2)
    
    def batch_process(self, root_folder, pattern="*.json", skip_existing=True):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰jsonæ–‡ä»¶"""
        print(f"ğŸ” æ‰«ææ–‡ä»¶å¤¹: {root_folder}")
        print(f"ğŸ“‹ æ–‡ä»¶æ¨¡å¼: {pattern}")
        print(f"ğŸ§µ çº¿ç¨‹æ•°: {self.max_workers}")
        
        # æŸ¥æ‰¾æ‰€æœ‰jsonæ–‡ä»¶
        json_files = self.find_all_json_files(root_folder, pattern)
        
        if not json_files:
            print(f"âŒ åœ¨ {root_folder} ä¸­æœªæ‰¾åˆ°åŒ¹é… {pattern} çš„æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
        
        # å¦‚æœè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œå…ˆè¿‡æ»¤ï¼ˆä½†è¦æ£€æŸ¥æ˜¯å¦å¤„ç†æˆåŠŸï¼‰
        if skip_existing:
            filtered_files = []
            for file_path in json_files:
                output_files = self.get_output_filename(file_path)
                # æ–‡ä»¶ä¸å­˜åœ¨æˆ–è€…å¤„ç†å¤±è´¥çš„éƒ½éœ€è¦å¤„ç†
                if (not output_files['summary_file'].exists() or 
                    not self.check_emotion_analysis_success(output_files['detailed_file'])):
                    filtered_files.append(file_path)
            
            skipped_count = len(json_files) - len(filtered_files)
            if skipped_count > 0:
                print(f"â­ï¸  è·³è¿‡ {skipped_count} ä¸ªå·²æˆåŠŸå¤„ç†çš„æ–‡ä»¶")
            
            json_files = filtered_files
        
        if not json_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²æˆåŠŸå¤„ç†å®Œæˆï¼")
            return
        
        print(f"ğŸš€ å¼€å§‹å¤„ç† {len(json_files)} ä¸ªæ–‡ä»¶...")
        
        # é‡ç½®è®¡æ•°å™¨
        self.processed_count = 0
        self.success_count = 0
        self.failed_count = 0
        self.reprocessed_count = 0
        self.start_time = time.time()
        
        # å¯åŠ¨è¿›åº¦æ˜¾ç¤ºçº¿ç¨‹
        progress_thread = threading.Thread(
            target=self.print_progress, 
            args=(len(json_files),),
            daemon=True
        )
        progress_thread.start()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ–‡ä»¶
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self.process_single_file, file_path, i % self.max_workers + 1): file_path 
                for i, file_path in enumerate(json_files)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    success, message = future.result()
                    results.append((success, message))
                except Exception as e:
                    results.append((False, f"çº¿ç¨‹å¼‚å¸¸: {file_path} - {str(e)}"))
        
        # ç­‰å¾…è¿›åº¦çº¿ç¨‹ç»“æŸ
        time.sleep(1)
        print()  # æ¢è¡Œ
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - self.start_time
        
        print(f"\nğŸ“Š æ‰¹é‡æƒ…ç»ªåˆ†æå®Œæˆï¼")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        print(f"âœ… æˆåŠŸ: {self.success_count}")
        print(f"âŒ å¤±è´¥: {self.failed_count}")
        print(f"ğŸ”„ é‡æ–°å¤„ç†: {self.reprocessed_count}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(self.success_count/len(json_files)*100):.1f}%")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        log_file = f"batch_emotion_analysis_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"æ‰¹é‡æƒ…ç»ªåˆ†ææ—¥å¿— - {datetime.now()}\n")
            f.write(f"æ ¹æ–‡ä»¶å¤¹: {root_folder}\n")
            f.write(f"æ–‡ä»¶æ¨¡å¼: {pattern}\n")
            f.write(f"çº¿ç¨‹æ•°: {self.max_workers}\n")
            f.write(f"æ€»è€—æ—¶: {total_time:.1f} ç§’\n")
            f.write(f"æˆåŠŸ: {self.success_count}/{len(json_files)}\n")
            f.write(f"é‡æ–°å¤„ç†: {self.reprocessed_count}\n\n")
            
            f.write("è¯¦ç»†ç»“æœ:\n")
            for success, message in results:
                status = "âœ…" if success else "âŒ"
                f.write(f"{status} {message}\n")
        
        print(f"ğŸ“ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

def main():
    parser = argparse.ArgumentParser(description='å¤šçº¿ç¨‹æ‰¹é‡æƒ…ç»ªåˆ†æå·¥å…·')
    parser.add_argument('root_folder', 
                       help='æ ¹æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å«å¤šä¸ªå­æ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('--config', '-c', 
                       default='emovoice_api_config.json',
                       help='APIé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: emovoice_api_config.jsonï¼‰')
    parser.add_argument('--api-type', 
                       choices=['openai', 'deepseek', 'claude', 'custom'],
                       help='æŒ‡å®šAPIç±»å‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰')
    parser.add_argument('--pattern', '-p',
                       default='*.json',
                       help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤: *.jsonï¼‰')
    parser.add_argument('--threads', '-t',
                       type=int,
                       default=4,
                       help='çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰')
    parser.add_argument('--no-skip-existing',
                       action='store_true',
                       help='ä¸è·³è¿‡å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ ¹æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.root_folder):
        print(f"âŒ æ ¹æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.root_folder}")
        return
    
    if not os.path.isdir(args.root_folder):
        print(f"âŒ è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {args.root_folder}")
        return
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = BatchEmotionProcessor(config=None, max_workers=args.threads)
    
    # åŠ è½½é…ç½®
    config = processor.load_config(args.config)
    if not config:
        return
    
    processor.config = config
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†APIç±»å‹ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    if args.api_type:
        config['api_type'] = args.api_type
    
    # æ£€æŸ¥APIå¯†é’¥
    api_type = config.get('api_type', 'openai')
    if api_type == 'openai' and not config.get('openai_api_key'):
        print("âŒ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®OpenAI APIå¯†é’¥")
        return
    elif api_type == 'deepseek' and not config.get('deepseek_api_key'):
        print("âŒ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®DeepSeek APIå¯†é’¥")
        return
    elif api_type == 'claude' and not config.get('claude_api_key'):
        print("âŒ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®Claude APIå¯†é’¥")
        return
    
    print(f"ğŸ”§ é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸš€ ä½¿ç”¨ {api_type.upper()} API")
    
    # å¼€å§‹æ‰¹é‡å¤„ç†
    processor.batch_process(
        root_folder=args.root_folder,
        pattern=args.pattern,
        skip_existing=not args.no_skip_existing
    )

if __name__ == "__main__":
    main()
